{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d6112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to remove the '#' from the following commands to install the required dependencies\n",
    "\n",
    "#these two are to read the excel\n",
    "#! pip install xlrd\n",
    "#! pip install install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcc1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c173fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_repetitions=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c511d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you forget to add the split number and the number of splits?\n"
     ]
    }
   ],
   "source": [
    "# Defining the number of splits in the data\n",
    "try:\n",
    "    split_nr = int(sys.argv[1])\n",
    "    n_splits = int(sys.argv[2])\n",
    "except:\n",
    "    split_nr = 1\n",
    "    n_splits = 10\n",
    "    print('Did you forget to add the split number and the number of splits?')\n",
    "    #exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d88f4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cell_Line', 'P37108.SRP14_HUMAN', 'Q96JP5.ZFP91_HUMAN',\n",
       "       'Q9Y4H2.IRS2_HUMAN', 'P36578.RL4_HUMAN', 'Q6SPF0.SAMD1_HUMAN',\n",
       "       'O76031.CLPX_HUMAN', 'Q8WUQ7.CATIN_HUMAN', 'A6NIH7.U119B_HUMAN',\n",
       "       'Q9BTD8.RBM42_HUMAN',\n",
       "       ...\n",
       "       'P33151.CADH5_HUMAN', 'Q5EBL4.RIPL1_HUMAN', 'P49715.CEBPA_HUMAN',\n",
       "       'Q5TA45.INT11_HUMAN', 'O14924.RGS12_HUMAN', 'Q7Z3B1.NEGR1_HUMAN',\n",
       "       'O60669.MOT2_HUMAN', 'Q13571.LAPM5_HUMAN', 'Q96JM2.ZN462_HUMAN',\n",
       "       'P35558.PCKGC_HUMAN'],\n",
       "      dtype='object', length=6693)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One row per cell line\n",
    "#DIR\n",
    "x = pd.read_excel('data/ProCan-DepMapSanger_protein_matrix_6692_averaged.xlsx', engine='openpyxl').fillna(0).drop(columns=['Project_Identifier'])\n",
    "c = [a.replace(';','.') for a in x.columns]\n",
    "x.columns = c\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['drug_id', 'cell_line_name', 'ln_IC50'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIR\n",
    "y = pd.read_csv('data/DrugResponse_PANCANCER_GDSC1_GDSC2_20200602.csv')[['drug_id','cell_line_name','ln_IC50']]\n",
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c38c3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is the function to go from the table to a JSON file (variantSpark format and structure)\n",
    "\n",
    "def merge_tree_node(tree, node):\n",
    "    \n",
    "    # Empty tree\n",
    "    if type(tree)==float: return tree\n",
    "    if len(tree)==0: return node\n",
    "\n",
    "    # Direct children\n",
    "    if tree['right'] == node['nodeID']:\n",
    "        tree['right'] = node\n",
    "        return tree\n",
    "    elif tree['left'] == node['nodeID']:\n",
    "        tree['left'] = node\n",
    "        return tree\n",
    "\n",
    "    # Create\n",
    "    right = merge_tree_node(tree['right'], node)\n",
    "    left = merge_tree_node(tree['left'], node)\n",
    "    tree['right'] = right\n",
    "    tree['left'] = left\n",
    "    return tree\n",
    "            \n",
    "\n",
    "def from_table_to_json(m):\n",
    "    tree = {}\n",
    "    for _id,row in m.iterrows():\n",
    "        current_node = {'nodeID': row['nodeID'], \n",
    "                        'splitvarID':row['splitvarID'],\n",
    "                        'splitVar':row['splitvarName'],\n",
    "                        'splitval':row['splitval'], \n",
    "                        'terminal':row['terminal'], \n",
    "                        'prediction':row['prediction'], \n",
    "                        'left':row['leftChild'], \n",
    "                        'right':row['rightChild'] }\n",
    "        tree = merge_tree_node(tree, current_node)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "#m = pd.read_csv('output/tree1.csv')\n",
    "#from_table_to_json(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e620d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to get the interacting nodes (no testing done yet)\n",
    "\n",
    "def get_interactions(tree, current_list, interactions):\n",
    "    if not 'splitVar' in tree.keys():\n",
    "        return 0\n",
    "    if str(tree['splitVar']) == 'nan': return 0 #ranger adds a fake predicting node at the end\n",
    "    \n",
    "    # Adding the interaction\n",
    "    current_list.append(tree['splitVar'])\n",
    "    if len(current_list) >= 2:\n",
    "        for i in range(2,len(current_list)+1):\n",
    "            aux = '+'.join(sorted(current_list[-i:]))\n",
    "            if aux in interactions.keys():\n",
    "                interactions[aux] +=1\n",
    "            else:\n",
    "                interactions[aux] = 1\n",
    "                    \n",
    "    if 'left' in tree.keys():\n",
    "        get_interactions(tree['left'], current_list, interactions)\n",
    "    if 'right' in tree.keys():\n",
    "        get_interactions(tree['right'], current_list, interactions)\n",
    "        \n",
    "    _ = current_list.pop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e31cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all the interactions\n",
    "\n",
    "def test_interactions(df, data):\n",
    "    \"\"\"\n",
    "    I use GLM because:\n",
    "    The main difference between the two approaches is that the general linear model strictly assumes that\n",
    "    the residuals will follow a conditionally normal distribution, while the GLM loosens this assumption \n",
    "    and allows for a variety of other distributions from the exponential family for the residuals.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "    counts = 0\n",
    "\n",
    "    for v in df[(df.repetitions>=2) & (df.order ==2)].variants.tolist():\n",
    "        \n",
    "        #preparing the input\n",
    "        sp=v.split('+')\n",
    "        xy = data[sp+['ln_IC50']].fillna(-1)\n",
    "        sp=v.replace('_','').split('+')\n",
    "        xy.columns = [''.join([chr(int(y)+97) if y.isnumeric() else y for y in x.replace('_','').replace('.','')]) for x in xy.columns]\n",
    "        formula = xy.columns[-1]+' ~ '\n",
    "        for i in range(1,len(xy.columns)):\n",
    "            formula = formula + ' + '.join(['*'.join(o) for o in list(combinations(xy.columns[:-1],i))])\n",
    "            formula = formula + ' + '\n",
    "        formula = formula.rstrip(' + ')\n",
    "\n",
    "        # Standard fitting\n",
    "        ols = smf.ols(formula,data=xy)\n",
    "        ols.raise_on_perfect_prediction = False #preventing the perfect separation error\n",
    "        results = ols.fit(disp=False, maxiter=1000) #mehtod prevents singular matrix\n",
    "        results = results.summary()\n",
    "        converged = results.tables[0].data[5][1].strip()\n",
    "        pseudo_r2 = results.tables[0].data[3][3].strip()\n",
    "        results = results.tables[1].data\n",
    "        results = pd.DataFrame(results[1:], columns=['coef_id', 'coef', 'std err', 'z', 'P>|z|', '[0.025', '0.975]'])\n",
    "        results['standard_fitting'] = True\n",
    "\n",
    "        #If nan means no convergence bc singular matrix\n",
    "        #adding regularization\n",
    "        if 'nan' == pd.DataFrame(results)['z'].iloc[2].strip():\n",
    "            try:\n",
    "                results = ols.fit_regularized(method='l1', disp=False, maxiter=1000, alpha=0.3) #mehtod prevents singular matrix\n",
    "                results = results.summary()\n",
    "                converged = results.tables[0].data[5][1].strip()\n",
    "                pseudo_r2 = results.tables[0].data[3][3].strip()\n",
    "                results = results.tables[1].data\n",
    "                results = pd.DataFrame(results[1:], columns=['coef_id', 'coef', 'std err', 'z', 'P>|z|', '[0.025', '0.975]'])\n",
    "                results['standard_fitting'] = False        \n",
    "            except:\n",
    "                #crashed the regularized\n",
    "                counts +=1\n",
    "\n",
    "        results['converged'] = converged\n",
    "        results['pseudo_r2'] = pseudo_r2\n",
    "        results['snps'] = v\n",
    "        results['order'] = len(sp)\n",
    "        final_results.append(results)\n",
    "\n",
    "    final_results = pd.concat(final_results)\n",
    "    #print(counts)\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e159f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all the interactions\n",
    "\n",
    "def results_fit_to_df(results):\n",
    "    coeffs = results.params.tolist()\n",
    "    pvals = results.pvalues.tolist()\n",
    "    pseudo_r2 = results.rsquared\n",
    "    tvals = results.tvalues.tolist()\n",
    "    cint_low = results.conf_int()[0].tolist()\n",
    "    cint_high = results.conf_int()[1].tolist()\n",
    "\n",
    "    results = results.summary()\n",
    "    converged = results.tables[0].data[5][1].strip()\n",
    "    results = results.tables[1].data\n",
    "    results = pd.DataFrame(results[1:], columns=['coef_id', 'coef', 'std err', 'z', 'P>|z|', '[0.025', '0.975]'])\n",
    "    results['P>|z|'] = pvals\n",
    "    results['z'] = tvals \n",
    "    results['coef'] = coeffs\n",
    "    results['converged'] = converged\n",
    "    results['pseudo_r2'] = pseudo_r2\n",
    "    results['[0.025'] = cint_low\n",
    "    results['0.975]'] = cint_high\n",
    "    return results\n",
    "    \n",
    "def test_interactions_high(df, data, max_order=4):\n",
    "    \"\"\"\n",
    "    I use GLM because:\n",
    "    The main difference between the two approaches is that the general linear model strictly assumes that\n",
    "    the residuals will follow a conditionally normal distribution, while the GLM loosens this assumption \n",
    "    and allows for a variety of other distributions from the exponential family for the residuals.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "    counts = 0\n",
    "\n",
    "    for m_or in range(2,max_order+1):\n",
    "        #print('current order',m_or)\n",
    "        \n",
    "        for v in df[(df.repetitions>=2) & (df.order==m_or)].variants.tolist():\n",
    "            #preparing the input\n",
    "            sp=v.split('+')\n",
    "            xy = data[sp+['ln_IC50']].fillna(-1)\n",
    "            sp=v.replace('_','').split('+')\n",
    "            xy.columns = [''.join([chr(int(y)+97) if y.isnumeric() else y for y in x.replace('_','').replace('.','')]) for x in xy.columns]\n",
    "            formula = xy.columns[-1]+' ~ '\n",
    "            for i in range(1,len(xy.columns)):\n",
    "                formula = formula + ' + '.join(['*'.join(o) for o in list(combinations(xy.columns[:-1],i))])\n",
    "                formula = formula + ' + '\n",
    "            formula = formula.rstrip(' + ')\n",
    "            \n",
    "            #Recreating the formula\n",
    "            if m_or>2:\n",
    "                #gathering all interactions\n",
    "                fs = formula.split(' + ')\n",
    "                formula = ' + '.join([a for a in fs if '*' not in a]+[a for a in fs if a.count('*')== m_or-1])\n",
    "                all_interactions = [a.replace('*',':') for a in fs if '*' in a]\n",
    "                final_results = pd.concat(final_results)\n",
    "                subset = final_results[final_results.coef_id.apply(lambda a: a in all_interactions)].reset_index(drop=True)\n",
    "                final_results = [final_results]\n",
    "                if len(subset)>0:\n",
    "                    max_idx = subset['coef'].astype(float).idxmax()\n",
    "                    coef_id = subset.loc[max_idx].coef_id\n",
    "                    formula = formula +' + '+coef_id.replace(':','*')\n",
    "                else:\n",
    "                    #pass\n",
    "                    continue # bc i dont think it is a valid tree form (interaction-wise)\n",
    "                    #There is no sub epistasis (P>Q>O>P, tree 503, first compound)\n",
    "\n",
    "            # Standard fitting\n",
    "            try:\n",
    "                ols = smf.ols(formula.replace('*',':'),data=xy)\n",
    "                # \"*\" vs \":\" #https://stackoverflow.com/questions/33050104/difference-between-the-interaction-and-term-for-formulas-in-statsmodels-ols\n",
    "            except:\n",
    "                print('error in OLS')\n",
    "                print('coef_id',coef_id)\n",
    "                print('formula OLS',type(formula),formula)\n",
    "                #return pd.concat(final_results)\n",
    "                continue\n",
    "            ols.raise_on_perfect_prediction = False #preventing the perfect separation error\n",
    "            results = ols.fit(disp=False, maxiter=1000) #mehtod prevents singular matrix\n",
    "#            return results\n",
    "            results = results_fit_to_df(results)\n",
    "            results['standard_fitting'] = True\n",
    "\n",
    "            #If nan means no convergence bc singular matrix\n",
    "            #adding regularization\n",
    "            if 'nan' == pd.DataFrame(results)['z'].astype(str).iloc[2].strip():\n",
    "                try:\n",
    "                    results = ols.fit_regularized(method='l1', disp=False, maxiter=1000, alpha=0.3) #mehtod prevents singular matrix\n",
    "                    results = results_fit_to_df(results)\n",
    "                    results['standard_fitting'] = False        \n",
    "                except:\n",
    "                    #crashed the regularized\n",
    "                    counts +=1\n",
    "                    continue\n",
    "\n",
    "\n",
    "            results['snps'] = v\n",
    "            results['order'] = len(sp)\n",
    "            final_results.append(results)\n",
    "\n",
    "    final_results = pd.concat(final_results)\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57ce16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo(string):\n",
    "    \n",
    "    string = ''.join([ x if ord(x)<90 else str(ord(x)-97) for x in string ])\n",
    "    string = string[:6]+'.'+string[6:].replace('HUMAN', '_HUMAN') #not sure these 6\n",
    "    return string\n",
    "    \n",
    "#undo('PacfbbCRYABHUMAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b0b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 75\n",
      "current order 2\n",
      "current order 3\n",
      "current order 4\n",
      "1 out of 75\n",
      "current order 2\n",
      "current order 3\n",
      "current order 4\n",
      "2 out of 75\n",
      "current order 2\n",
      "current order 3\n",
      "current order 4\n"
     ]
    }
   ],
   "source": [
    "#TODO: By compound\n",
    "\n",
    "#Looping over all drugs\n",
    "# drug_id\n",
    "# Other options:\n",
    "# - drug_name\n",
    "# - CHEMBL = Chemical compound ID\n",
    "#for compound_name, group in x.merge(y, left_on='Cell_Line', right_on='cell_line_name').groupby('drug_id'): # may require too much memory\n",
    "\n",
    "#Making a temp file to run all R stuff\n",
    "#DIR\n",
    "os.system(f\"mkdir -p tmp/tmp{split_nr}\")\n",
    "\n",
    "\n",
    "column_to_group = 'drug_id'\n",
    "drugs_list = y[column_to_group].sort_values().unique()\n",
    "drugs_list = [drugs_list[i] for i in range(len(drugs_list)) if i%n_splits==split_nr]\n",
    "i = -1\n",
    "all_drug_results = []\n",
    "for elm in drugs_list[:3]:\n",
    "    i+=1\n",
    "    \n",
    "    if i%10==0 or i<10: print(i,'out of',len(drugs_list), 'drugs in split nr', split_nr)\n",
    "\n",
    "    xy = x.merge(y[y[column_to_group]==elm], left_on='Cell_Line', right_on='cell_line_name')\n",
    "    #Enhancement: Remove peptides that are all zero \n",
    "    \n",
    "    # saving csv for R df\n",
    "    # file name is generic but we could personalize it\n",
    "    #DIR\n",
    "    xy.drop(columns=['Cell_Line', 'cell_line_name','drug_id']).rename(columns={'ln_IC50':'label'}).to_csv(f\"tmp/tmp{split_nr}/data.csv\", index=False)\n",
    "\n",
    "    #Run the R script to generate the outputs\n",
    "    os.system(f\"Rscript ranger_run.R {split_nr}\")\n",
    "    \n",
    "    #load the R outputs (the trees, one file each), and convert it to VS look-alike and get interactions\n",
    "    interactions = {}\n",
    "    #DIR\n",
    "    trees = os.listdir(f\"tmp/tmp{split_nr}/\")\n",
    "    #files = [x for x in files if 'tree' in x]\n",
    "    for tree in trees:\n",
    "        if 'tree' not in tree: continue #if it is not a tree file ignore\n",
    "        #DIR\n",
    "        tree_json = from_table_to_json(pd.read_csv(f\"tmp/tmp{split_nr}/\"+tree))        \n",
    "        get_interactions(tree_json,[],interactions) #the interactions are found in \"interactions\"\n",
    "        \n",
    "    # Creating a df out of the interactions\n",
    "    df = pd.DataFrame({'variants':interactions.keys(),'repetitions':interactions.values()})\n",
    "    df['order'] = df.variants.apply(lambda x: x.count('+')+1)\n",
    "    \n",
    "    \n",
    "    tested_interactions = test_interactions_high(df, xy) #here you define which order of interactions you want to compute\n",
    "    tested_interactions['drug'] = elm\n",
    "    all_drug_results.append(tested_interactions)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "final_results = pd.concat(all_drug_results)\n",
    "#DIR\n",
    "final_results.to_csv(f\"tmp/final_results{split_nr}.tsv\", index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "073445d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 1\n"
     ]
    }
   ],
   "source": [
    "print('Done:',split_nr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
