{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d6112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to remove the '#' from the following commands to install the required dependencies\n",
    "\n",
    "#these two are to read the excel\n",
    "#! pip install xlrd\n",
    "#! pip install install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c173fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ***** THESE ARE THE DEFAULTS UNLESS THEY ARE CHANGED WHEN YOU RUN THE CODE!!! *****\n",
    "\n",
    "min_repetitions = 2 #Number of repetitions an interaction appears in the trees\n",
    "max_order = 4 \n",
    "working_dir = 'tmp/' #make sure it is empty\n",
    "\n",
    "#Ranger parameters\n",
    "n_trees = 1000 #Number of trees\n",
    "mtry = 100 # Number of variables to possibly split at in each node. Default is the (rounded down) square root of the number variables. \n",
    "max_depth = 0# Maximal tree depth. A value of NULL or 0 (the default) corresponds to unlimited depth, 1 to tree stumps (1 split per tree).\n",
    "min_node= 5 # Minimal node size. default ranger: 5\n",
    "\n",
    "# File inputs\n",
    "x_input = 'data/ProCan-DepMapSanger_protein_matrix_6692_averaged.xlsx'\n",
    "y_input = 'data/DrugResponse_PANCANCER_GDSC1_GDSC2_20200602.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fb1323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--split SPLIT] [--nSplits NSPLITS]\n",
      "                             [--minRep MINREP] [--maxOrder MAXORDER]\n",
      "                             [--nTrees NTREES] [--mtry MTRY]\n",
      "                             [--maxDepth MAXDEPTH] [--minNode MINNODE]\n",
      "                             [--workingDir WORKINGDIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/reg032/Library/Jupyter/runtime/kernel-aca244b8-0278-4f43-9a65-2aa2536954c8.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Defining the number of splits in the data\n",
    "try:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--split', default=1)\n",
    "    parser.add_argument('--nSplits', default=1) #This makes it not parallel\n",
    "    parser.add_argument('--minRep', default=min_repetitions)\n",
    "    parser.add_argument('--maxOrder', default=max_order)\n",
    "    parser.add_argument('--nTrees', default=n_trees)\n",
    "    parser.add_argument('--mtry', default=mtry)\n",
    "    parser.add_argument('--maxDepth', default=max_depth)\n",
    "    parser.add_argument('--minNode', default=min_node)\n",
    "    parser.add_argument('--workingDir', default=working_dir)\n",
    "    \n",
    "    \n",
    "    parser_args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    split_nr = int(parser_args.split)\n",
    "    n_splits = int(parser_args.nSplits)\n",
    "    \n",
    "    \n",
    "    min_repetitions = int(parser_args.minRep) \n",
    "    max_order = int(parser_args.maxOrder)\n",
    "    working_dir = parser_args.workingDir\n",
    "\n",
    "    #Ranger parameters\n",
    "    n_trees = int(parser_args.nTrees)\n",
    "    mtry = int(parser_args.mtry)\n",
    "    max_depth = int(parser_args.maxDepth)\n",
    "    min_node= int(parser_args.minNode)\n",
    "\n",
    "\n",
    "except Exception as e: \n",
    "    print(e)\n",
    "    split_nr = 1\n",
    "    n_splits = 2\n",
    "    print('Did you forget to add the split number and the number of splits?')\n",
    "    #exit()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144e4540",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_nr = 1\n",
    "n_splits = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf5e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this split finalized successfully, it does not re-run\n",
    "if os.path.exists(working_dir+f\"final_results{split_nr}.tsv\") and os.path.exists(working_dir+f\"tree_performances{split_nr}.tsv\"):\n",
    "    print('Job previously run successfully!\\nExiting')\n",
    "    exit()\n",
    "    \n",
    "if os.path.exists(working_dir+f\"final_results.tsv\") and os.path.exists(working_dir+f\"tree_performances.tsv\"):\n",
    "    print('Job previously run successfully!\\nExiting')\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6a0db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easing things for other systems\n",
    "dir_trees_tmp = working_dir+\"tmp\" # temportal directory where each of the separate trees are going to be saved\n",
    "#final_results_dir =\"tmp/\" # directory where the final results are going to be saved\n",
    "path_to_R = 'Rscript' # Path to run R\n",
    "path_to_ranger_script = 'ranger_run-parallel.R' # Path to the ranger script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d88f4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cell_Line', 'P37108.SRP14_HUMAN', 'Q96JP5.ZFP91_HUMAN',\n",
       "       'Q9Y4H2.IRS2_HUMAN', 'P36578.RL4_HUMAN', 'Q6SPF0.SAMD1_HUMAN',\n",
       "       'O76031.CLPX_HUMAN', 'Q8WUQ7.CATIN_HUMAN', 'A6NIH7.U119B_HUMAN',\n",
       "       'Q9BTD8.RBM42_HUMAN',\n",
       "       ...\n",
       "       'P33151.CADH5_HUMAN', 'Q5EBL4.RIPL1_HUMAN', 'P49715.CEBPA_HUMAN',\n",
       "       'Q5TA45.INT11_HUMAN', 'O14924.RGS12_HUMAN', 'Q7Z3B1.NEGR1_HUMAN',\n",
       "       'O60669.MOT2_HUMAN', 'Q13571.LAPM5_HUMAN', 'Q96JM2.ZN462_HUMAN',\n",
       "       'P35558.PCKGC_HUMAN'],\n",
       "      dtype='object', length=6693)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One row per cell line\n",
    "#DIR\n",
    "x = pd.read_excel(x_input, engine='openpyxl').drop(columns=['Project_Identifier'])\n",
    "c = [a.replace(';','.') for a in x.columns]\n",
    "x.columns = c\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['drug_id', 'cell_line_name', 'ln_IC50'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIR\n",
    "y = pd.read_csv(y_input)[['drug_id','cell_line_name','ln_IC50']]\n",
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c38c3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is the function to go from the table to a JSON file (variantSpark format and structure)\n",
    "\n",
    "def merge_tree_node(tree, node):\n",
    "    \n",
    "    # Empty tree\n",
    "    if type(tree)==float: return tree\n",
    "    if len(tree)==0: return node\n",
    "\n",
    "    # Direct children\n",
    "    if tree['right'] == node['nodeID']:\n",
    "        tree['right'] = node\n",
    "        return tree\n",
    "    elif tree['left'] == node['nodeID']:\n",
    "        tree['left'] = node\n",
    "        return tree\n",
    "\n",
    "    # Create\n",
    "    right = merge_tree_node(tree['right'], node)\n",
    "    left = merge_tree_node(tree['left'], node)\n",
    "    tree['right'] = right\n",
    "    tree['left'] = left\n",
    "    return tree\n",
    "            \n",
    "\n",
    "def from_table_to_json(m):\n",
    "    tree = {}\n",
    "    for _id,row in m.iterrows():\n",
    "        current_node = {'nodeID': row['nodeID'], \n",
    "                        'splitvarID':row['splitvarID'],\n",
    "                        'splitVar':row['splitvarName'],\n",
    "                        'splitval':row['splitval'], \n",
    "                        'terminal':row['terminal'], \n",
    "                        'prediction':row['prediction'], \n",
    "                        'left':row['leftChild'], \n",
    "                        'right':row['rightChild'] }\n",
    "        tree = merge_tree_node(tree, current_node)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "#m = pd.read_csv('output/tree1.csv')\n",
    "#from_table_to_json(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e620d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to get the interacting nodes (no testing done yet)\n",
    "\n",
    "def get_interactions(tree, current_list, interactions):\n",
    "    if not 'splitVar' in tree.keys():\n",
    "        return 0\n",
    "    if str(tree['splitVar']) == 'nan': return 0 #ranger adds a fake predicting node at the end\n",
    "    \n",
    "    # Adding the interaction\n",
    "    current_list.append(tree['splitVar'])\n",
    "    if len(current_list) >= 2:\n",
    "        for i in range(2,len(current_list)+1):\n",
    "            aux = '+'.join(sorted(current_list[-i:]))\n",
    "            if aux in interactions.keys():\n",
    "                interactions[aux] +=1\n",
    "            else:\n",
    "                interactions[aux] = 1\n",
    "                    \n",
    "    if 'left' in tree.keys():\n",
    "        get_interactions(tree['left'], current_list, interactions)\n",
    "    if 'right' in tree.keys():\n",
    "        get_interactions(tree['right'], current_list, interactions)\n",
    "        \n",
    "    _ = current_list.pop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e159f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all the interactions\n",
    "\n",
    "def results_fit_to_df(results):\n",
    "    coeffs = results.params.tolist()\n",
    "    pvals = results.pvalues.tolist()\n",
    "    pseudo_r2 = results.rsquared\n",
    "    tvals = results.tvalues.tolist()\n",
    "    cint_low = results.conf_int()[0].tolist()\n",
    "    cint_high = results.conf_int()[1].tolist()\n",
    "\n",
    "    results = results.summary()\n",
    "    converged = results.tables[0].data[5][1].strip()\n",
    "    results = results.tables[1].data\n",
    "    results = pd.DataFrame(results[1:], columns=['coef_id', 'coef', 'std err', 'z', 'P>|z|', '[0.025', '0.975]'])\n",
    "    results['P>|z|'] = pvals\n",
    "    results['z'] = tvals \n",
    "    results['coef'] = coeffs\n",
    "    results['converged'] = converged\n",
    "    results['pseudo_r2'] = pseudo_r2\n",
    "    results['[0.025'] = cint_low\n",
    "    results['0.975]'] = cint_high\n",
    "    return results\n",
    "    \n",
    "def test_interactions_high(df, data, max_order=4, repetitions_threshold=2, min_samples=20):\n",
    "    \"\"\"\n",
    "    I use GLM because:\n",
    "    The main difference between the two approaches is that the general linear model strictly assumes that\n",
    "    the residuals will follow a conditionally normal distribution, while the GLM loosens this assumption \n",
    "    and allows for a variety of other distributions from the exponential family for the residuals.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "    counts = 0\n",
    "\n",
    "    for m_or in range(2,max_order+1):\n",
    "        #print('current order',m_or)\n",
    "        \n",
    "        for v in df[(df.repetitions>=2) & (df.order==m_or)].variants.tolist():\n",
    "            #preparing the input\n",
    "            sp=v.split('+')\n",
    "            xy = data[sp+['ln_IC50']].dropna()#.fillna(-1)\n",
    "            if len(xy.columns) <= m_or: continue #not enough columns\n",
    "            if len(xy) <= min_samples: continue #not enough rows\n",
    "            sp=v.replace('_','').split('+')\n",
    "            xy.columns = [''.join([chr(int(y)+97) if y.isnumeric() else y for y in x.replace('_','').replace('.','')]) for x in xy.columns]\n",
    "            formula = xy.columns[-1]+' ~ '\n",
    "            for i in range(1,len(xy.columns)):\n",
    "                formula = formula + ' + '.join(['*'.join(o) for o in list(combinations(xy.columns[:-1],i))])\n",
    "                formula = formula + ' + '\n",
    "            formula = formula.rstrip(' + ')\n",
    "            \n",
    "            #Recreating the formula\n",
    "            if m_or>2:\n",
    "                #gathering all interactions\n",
    "                fs = formula.split(' + ')\n",
    "                formula = ' + '.join([a for a in fs if '*' not in a]+[a for a in fs if a.count('*')== m_or-1])\n",
    "                all_interactions = [a.replace('*',':') for a in fs if '*' in a]\n",
    "                final_results = pd.concat(final_results)\n",
    "                subset = final_results[final_results.coef_id.apply(lambda a: a in all_interactions)].reset_index(drop=True)\n",
    "                final_results = [final_results]\n",
    "                if len(subset)>0:\n",
    "                    max_idx = subset['coef'].astype(float).abs().idxmax()\n",
    "                    coef_id = subset.loc[max_idx].coef_id\n",
    "                    formula = formula +' + '+coef_id.replace(':','*')\n",
    "                else:\n",
    "                    #pass\n",
    "                    continue # bc i dont think it is a valid tree form (interaction-wise)\n",
    "                    #There is no sub epistasis (P>Q>O>P, tree 503, first compound)\n",
    "\n",
    "            # Standard fitting\n",
    "            try:\n",
    "                ols = smf.ols(formula.replace('*',':'),data=xy)\n",
    "                # \"*\" vs \":\" #https://stackoverflow.com/questions/33050104/difference-between-the-interaction-and-term-for-formulas-in-statsmodels-ols\n",
    "            except:\n",
    "                print('error in OLS')\n",
    "                print('coef_id',coef_id)\n",
    "                print('formula OLS',type(formula),formula)\n",
    "                #return pd.concat(final_results)\n",
    "                continue\n",
    "            ols.raise_on_perfect_prediction = False #preventing the perfect separation error\n",
    "            results = ols.fit(disp=False, maxiter=1000) #mehtod prevents singular matrix\n",
    "#            return results\n",
    "            results = results_fit_to_df(results)\n",
    "            results['standard_fitting'] = True\n",
    "\n",
    "            #If nan means no convergence bc singular matrix\n",
    "            #adding regularization\n",
    "            if 'nan' == pd.DataFrame(results)['z'].astype(str).iloc[2].strip():\n",
    "                try:\n",
    "                    results = ols.fit_regularized(method='l1', disp=False, maxiter=1000, alpha=0.3) #mehtod prevents singular matrix\n",
    "                    results = results_fit_to_df(results)\n",
    "                    results['standard_fitting'] = False        \n",
    "                except:\n",
    "                    #crashed the regularized\n",
    "                    counts +=1\n",
    "                    continue\n",
    "\n",
    "\n",
    "            results['snps'] = v\n",
    "            results['order'] = len(sp)\n",
    "            final_results.append(results)\n",
    "\n",
    "    final_results = pd.concat(final_results)\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57ce16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo(string):\n",
    "    \n",
    "    string = ''.join([ x if ord(x)<90 else str(ord(x)-97) for x in string ])\n",
    "    string = string[:6]+'.'+string[6:].replace('HUMAN', '_HUMAN') #not sure these 6\n",
    "    return string\n",
    "    \n",
    "#undo('PacfbbCRYABHUMAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01b0b7fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rscript ranger_run-parallel.R  -w tmp/ -c 1 -n 1000 -t 100 -s 5 -d 0\n",
      "0 out of 374 drugs in split nr 1\n",
      "R output (in case there is an error or something\n",
      "\n",
      "Rscript ranger_run-parallel.R  -w tmp/ -c 1 -n 1000 -t 100 -s 5 -d 0\n",
      "1 out of 374 drugs in split nr 1\n",
      "R output (in case there is an error or something\n",
      "\n",
      "Rscript ranger_run-parallel.R  -w tmp/ -c 1 -n 1000 -t 100 -s 5 -d 0\n",
      "2 out of 374 drugs in split nr 1\n",
      "R output (in case there is an error or something\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TODO: By compound\n",
    "\n",
    "#Looping over all drugs\n",
    "# drug_id\n",
    "# Other options:\n",
    "# - drug_name\n",
    "# - CHEMBL = Chemical compound ID\n",
    "#for compound_name, group in x.merge(y, left_on='Cell_Line', right_on='cell_line_name').groupby('drug_id'): # may require too much memory\n",
    "\n",
    "#Making a temp file to run all R stuff\n",
    "#DIR\n",
    "os.system(\"mkdir -p \"+dir_trees_tmp+f\"{split_nr}\")\n",
    "\n",
    "\n",
    "column_to_group = 'drug_id'\n",
    "drugs_list = y[column_to_group].sort_values().unique()\n",
    "drugs_list = [drugs_list[i] for i in range(len(drugs_list)) if i%n_splits==split_nr-1]\n",
    "i = -1\n",
    "all_drug_results = []\n",
    "tree_performances = []\n",
    "for elm in drugs_list:\n",
    "    print(f\"{path_to_R} {path_to_ranger_script}  -w {working_dir} -c {split_nr} -n {n_trees} -t {mtry} -s {min_node} -d {max_depth}\")\n",
    "    i+=1\n",
    "    \n",
    "    if i%10==0 or i<10: print(i,'out of',len(drugs_list), 'drugs in split nr', split_nr)\n",
    "\n",
    "    xy = x.merge(y[y[column_to_group]==elm], left_on='Cell_Line', right_on='cell_line_name')\n",
    "    #Enhancement: Remove peptides that are all zero \n",
    "    \n",
    "    # saving csv for R df\n",
    "    # file name is generic but we could personalize it\n",
    "    #DIR\n",
    "    xy.drop(columns=['Cell_Line', 'cell_line_name','drug_id']).fillna(0).rename(columns={'ln_IC50':'label'}).to_csv(dir_trees_tmp+f\"{split_nr}/data.csv\", index=False)\n",
    "\n",
    "    #Run the R script to generate the outputs\n",
    "    #os.system(f\"{path_to_R} {path_to_ranger_script}  -w {working_dir} -c {split_nr} -n {n_trees} -t {mtry} -s {min_node} -d {max_depth}\")\n",
    "    print(\"R output (in case there is an error or something\")\n",
    "    #os.popen(f\"{path_to_R} {path_to_ranger_script}  -w {working_dir} -c {split_nr} -n {n_trees} -t {mtry} -s {min_node} -d {max_depth}\").read()    \n",
    "    print(os.popen(f\"{path_to_R} {path_to_ranger_script}  -w {working_dir} -c {split_nr} -n {n_trees} -t {mtry} -s {min_node} -d {max_depth}\").read())\n",
    "    \n",
    "    #load the R outputs (the trees, one file each), and convert it to VS look-alike and get interactions\n",
    "    interactions = {}\n",
    "  \n",
    "    aggregated_trees_df = pd.read_csv(os.path.join(dir_trees_tmp+str(split_nr),'aggregated_trees.csv'))\n",
    "    aggregated_trees_list = aggregated_trees_df.tree.unique()\n",
    "    for tree_nr in aggregated_trees_list:\n",
    "        tree_df = aggregated_trees_df[aggregated_trees_df.tree==tree_nr]\n",
    "        tree_json = from_table_to_json(tree_df)        \n",
    "        get_interactions(tree_json,[],interactions)\n",
    "    \n",
    "    # Creating a df out of the interactions\n",
    "    df = pd.DataFrame({'variants':interactions.keys(),'repetitions':interactions.values()})\n",
    "    df['order'] = df.variants.apply(lambda x: x.count('+')+1)\n",
    "    \n",
    "    #get tree performances\n",
    "    aux_performances = pd.read_csv(os.path.join(dir_trees_tmp+str(split_nr),\"performance.tsv\"), sep='\\t')\n",
    "    aux_performances['drug'] = elm\n",
    "    if os.path.isfile(working_dir+f\"tree_performances{split_nr}.tsv\"):\n",
    "        aux_performances.to_csv(working_dir+f\"tree_performances{split_nr}.tsv\", index=False, sep='\\t',mode='a', header=False)\n",
    "    else:\n",
    "        aux_performances.to_csv(working_dir+f\"tree_performances{split_nr}.tsv\", index=False, sep='\\t',mode='a')\n",
    "    \n",
    "    tested_interactions = test_interactions_high(df, xy, max_order=max_order, repetitions_threshold=min_repetitions) #here you define which order of interactions you want to compute\n",
    "    tested_interactions['drug'] = elm\n",
    "    if os.path.isfile(working_dir+f\"final_results{split_nr}.tsv\"):\n",
    "        tested_interactions.to_csv(working_dir+f\"final_results{split_nr}.tsv\", index=False, sep='\\t',mode='a', header=False)\n",
    "    else:\n",
    "        tested_interactions.to_csv(working_dir+f\"final_results{split_nr}.tsv\", index=False, sep='\\t',mode='a')\n",
    "    if i==2: break\n",
    "    \n",
    "\n",
    "\n",
    "# deliting temp folder from raneger\n",
    "shutil.rmtree(dir_trees_tmp+f\"{split_nr}\")\n",
    "#Option B: os.system(\"rm -rf _path_to_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b111aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split  1 out of  2  is DONE\n"
     ]
    }
   ],
   "source": [
    "print('Split ',split_nr, 'out of ',n_splits,' is DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fd0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All jobs finished\n",
    "for i in range(1,n_splits+1):\n",
    "    if os.path.exists('data'): exit() #Not all jobs finished\n",
    "    \n",
    "\n",
    "fr = [x for x in os.listdir(working_dir) if 'final_results' in x and 'final_results_all.tsv' not in x] #all jobs created the file\n",
    "if len(fr) == n_splits:\n",
    "\n",
    "    #appending everything together and Removing temp final results\n",
    "    for final_result in fr:\n",
    "        pd.read_csv(os.path.join(working_dir,final_result), sep='\\t').to_csv(working_dir+f\"final_results_all.tsv\", index=False, sep='\\t',mode='a')\n",
    "        os.remove(os.path.join(working_dir,final_result))\n",
    "        #pass\n",
    "        \n",
    "    # now the same for performances    \n",
    "    #Adding tree performances and Removing temp final results\n",
    "    pr = [x for x in os.listdir(working_dir) if 'tree_performances' in x and 'tree_performances_all.tsv' not in x]\n",
    "    for tree_performance in pr:\n",
    "        pd.read_csv(os.path.join(working_dir,tree_performance), sep='\\t').to_csv(working_dir+f\"tree_performances_all.tsv\", index=False, sep='\\t',mode='a')\n",
    "        os.remove(os.path.join(working_dir,tree_performance))\n",
    "        #pass\n",
    "\n",
    "    print('All jobs finished successfully!\\n final_results_all.tsv has all the aggregated output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
