{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d6112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to remove the '#' from the following commands to install the required dependencies\n",
    "\n",
    "#these two are to read the excel\n",
    "#! pip install xlrd\n",
    "#! pip install install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c173fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_repetitions = 2 #Number of repetitions an interaction appears in the trees\n",
    "max_order = 4 \n",
    "working_dir = 'tmp_2/' #make sure it is empty\n",
    "\n",
    "#Ranger parameters\n",
    "n_trees = 1000 #Number of trees\n",
    "mtry = 100 # Number of variables to possibly split at in each node. Default is the (rounded down) square root of the number variables. \n",
    "max_depth = 0# Maximal tree depth. A value of NULL or 0 (the default) corresponds to unlimited depth, 1 to tree stumps (1 split per tree).\n",
    "min_node= 5 # Minimal node size. default ranger: 5\n",
    "\n",
    "# File inputs\n",
    "x_input = 'data/ProCan-DepMapSanger_protein_matrix_6692_averaged.xlsx'\n",
    "y_input = 'data/DrugResponse_PANCANCER_GDSC1_GDSC2_20200602.csv'\n",
    "\n",
    "\n",
    "# Easing things for other systems\n",
    "dir_trees_tmp = working_dir+\"tmp\" # temportal directory where each of the separate trees are going to be saved\n",
    "#final_results_dir =\"tmp/\" # directory where the final results are going to be saved\n",
    "path_to_R = 'Rscript' # Path to run R\n",
    "path_to_ranger_script = 'ranger_run-parallel.R' # Path to the ranger script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc1cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23fb1323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did you forget to add the split number and the number of splits?\n"
     ]
    }
   ],
   "source": [
    "# Defining the number of splits in the data\n",
    "try:\n",
    "    split_nr = int(sys.argv[1])\n",
    "    n_splits = int(sys.argv[2])\n",
    "except:\n",
    "    split_nr = 1\n",
    "    n_splits = 2\n",
    "    print('Did you forget to add the split number and the number of splits?')\n",
    "    #exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d88f4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Cell_Line', 'P37108.SRP14_HUMAN', 'Q96JP5.ZFP91_HUMAN',\n",
       "       'Q9Y4H2.IRS2_HUMAN', 'P36578.RL4_HUMAN', 'Q6SPF0.SAMD1_HUMAN',\n",
       "       'O76031.CLPX_HUMAN', 'Q8WUQ7.CATIN_HUMAN', 'A6NIH7.U119B_HUMAN',\n",
       "       'Q9BTD8.RBM42_HUMAN',\n",
       "       ...\n",
       "       'P33151.CADH5_HUMAN', 'Q5EBL4.RIPL1_HUMAN', 'P49715.CEBPA_HUMAN',\n",
       "       'Q5TA45.INT11_HUMAN', 'O14924.RGS12_HUMAN', 'Q7Z3B1.NEGR1_HUMAN',\n",
       "       'O60669.MOT2_HUMAN', 'Q13571.LAPM5_HUMAN', 'Q96JM2.ZN462_HUMAN',\n",
       "       'P35558.PCKGC_HUMAN'],\n",
       "      dtype='object', length=6693)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One row per cell line\n",
    "#DIR\n",
    "x = pd.read_excel(x_input, engine='openpyxl').drop(columns=['Project_Identifier'])\n",
    "c = [a.replace(';','.') for a in x.columns]\n",
    "x.columns = c\n",
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d6587b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3185: DtypeWarning: Columns (24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['drug_id', 'cell_line_name', 'ln_IC50'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DIR\n",
    "y = pd.read_csv(y_input)[['drug_id','cell_line_name','ln_IC50']]\n",
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c38c3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is the function to go from the table to a JSON file (variantSpark format and structure)\n",
    "\n",
    "def merge_tree_node(tree, node):\n",
    "    \n",
    "    # Empty tree\n",
    "    if type(tree)==float: return tree\n",
    "    if len(tree)==0: return node\n",
    "\n",
    "    # Direct children\n",
    "    if tree['right'] == node['nodeID']:\n",
    "        tree['right'] = node\n",
    "        return tree\n",
    "    elif tree['left'] == node['nodeID']:\n",
    "        tree['left'] = node\n",
    "        return tree\n",
    "\n",
    "    # Create\n",
    "    right = merge_tree_node(tree['right'], node)\n",
    "    left = merge_tree_node(tree['left'], node)\n",
    "    tree['right'] = right\n",
    "    tree['left'] = left\n",
    "    return tree\n",
    "            \n",
    "\n",
    "def from_table_to_json(m):\n",
    "    tree = {}\n",
    "    for _id,row in m.iterrows():\n",
    "        current_node = {'nodeID': row['nodeID'], \n",
    "                        'splitvarID':row['splitvarID'],\n",
    "                        'splitVar':row['splitvarName'],\n",
    "                        'splitval':row['splitval'], \n",
    "                        'terminal':row['terminal'], \n",
    "                        'prediction':row['prediction'], \n",
    "                        'left':row['leftChild'], \n",
    "                        'right':row['rightChild'] }\n",
    "        tree = merge_tree_node(tree, current_node)\n",
    "    return tree\n",
    "\n",
    "\n",
    "\n",
    "# Test\n",
    "#m = pd.read_csv('output/tree1.csv')\n",
    "#from_table_to_json(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e620d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm to get the interacting nodes (no testing done yet)\n",
    "\n",
    "def get_interactions(tree, current_list, interactions):\n",
    "    if not 'splitVar' in tree.keys():\n",
    "        return 0\n",
    "    if str(tree['splitVar']) == 'nan': return 0 #ranger adds a fake predicting node at the end\n",
    "    \n",
    "    # Adding the interaction\n",
    "    current_list.append(tree['splitVar'])\n",
    "    if len(current_list) >= 2:\n",
    "        for i in range(2,len(current_list)+1):\n",
    "            aux = '+'.join(sorted(current_list[-i:]))\n",
    "            if aux in interactions.keys():\n",
    "                interactions[aux] +=1\n",
    "            else:\n",
    "                interactions[aux] = 1\n",
    "                    \n",
    "    if 'left' in tree.keys():\n",
    "        get_interactions(tree['left'], current_list, interactions)\n",
    "    if 'right' in tree.keys():\n",
    "        get_interactions(tree['right'], current_list, interactions)\n",
    "        \n",
    "    _ = current_list.pop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e159f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing all the interactions\n",
    "\n",
    "def results_fit_to_df(results):\n",
    "    coeffs = results.params.tolist()\n",
    "    pvals = results.pvalues.tolist()\n",
    "    pseudo_r2 = results.rsquared\n",
    "    tvals = results.tvalues.tolist()\n",
    "    cint_low = results.conf_int()[0].tolist()\n",
    "    cint_high = results.conf_int()[1].tolist()\n",
    "\n",
    "    results = results.summary()\n",
    "    converged = results.tables[0].data[5][1].strip()\n",
    "    results = results.tables[1].data\n",
    "    results = pd.DataFrame(results[1:], columns=['coef_id', 'coef', 'std err', 'z', 'P>|z|', '[0.025', '0.975]'])\n",
    "    results['P>|z|'] = pvals\n",
    "    results['z'] = tvals \n",
    "    results['coef'] = coeffs\n",
    "    results['converged'] = converged\n",
    "    results['pseudo_r2'] = pseudo_r2\n",
    "    results['[0.025'] = cint_low\n",
    "    results['0.975]'] = cint_high\n",
    "    return results\n",
    "    \n",
    "def test_interactions_high(df, data, max_order=4):\n",
    "    \"\"\"\n",
    "    I use GLM because:\n",
    "    The main difference between the two approaches is that the general linear model strictly assumes that\n",
    "    the residuals will follow a conditionally normal distribution, while the GLM loosens this assumption \n",
    "    and allows for a variety of other distributions from the exponential family for the residuals.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "    counts = 0\n",
    "\n",
    "    for m_or in range(2,max_order+1):\n",
    "        #print('current order',m_or)\n",
    "        \n",
    "        for v in df[(df.repetitions>=2) & (df.order==m_or)].variants.tolist():\n",
    "            #preparing the input\n",
    "            sp=v.split('+')\n",
    "            xy = data[sp+['ln_IC50']].dropna()#.fillna(-1)\n",
    "            if len(xy) <2: continue\n",
    "            sp=v.replace('_','').split('+')\n",
    "            xy.columns = [''.join([chr(int(y)+97) if y.isnumeric() else y for y in x.replace('_','').replace('.','')]) for x in xy.columns]\n",
    "            formula = xy.columns[-1]+' ~ '\n",
    "            for i in range(1,len(xy.columns)):\n",
    "                formula = formula + ' + '.join(['*'.join(o) for o in list(combinations(xy.columns[:-1],i))])\n",
    "                formula = formula + ' + '\n",
    "            formula = formula.rstrip(' + ')\n",
    "            \n",
    "            #Recreating the formula\n",
    "            if m_or>2:\n",
    "                #gathering all interactions\n",
    "                fs = formula.split(' + ')\n",
    "                formula = ' + '.join([a for a in fs if '*' not in a]+[a for a in fs if a.count('*')== m_or-1])\n",
    "                all_interactions = [a.replace('*',':') for a in fs if '*' in a]\n",
    "                final_results = pd.concat(final_results)\n",
    "                subset = final_results[final_results.coef_id.apply(lambda a: a in all_interactions)].reset_index(drop=True)\n",
    "                final_results = [final_results]\n",
    "                if len(subset)>0:\n",
    "                    max_idx = subset['coef'].astype(float).abs().idxmax()\n",
    "                    coef_id = subset.loc[max_idx].coef_id\n",
    "                    formula = formula +' + '+coef_id.replace(':','*')\n",
    "                else:\n",
    "                    #pass\n",
    "                    continue # bc i dont think it is a valid tree form (interaction-wise)\n",
    "                    #There is no sub epistasis (P>Q>O>P, tree 503, first compound)\n",
    "\n",
    "            # Standard fitting\n",
    "            try:\n",
    "                ols = smf.ols(formula.replace('*',':'),data=xy)\n",
    "                # \"*\" vs \":\" #https://stackoverflow.com/questions/33050104/difference-between-the-interaction-and-term-for-formulas-in-statsmodels-ols\n",
    "            except:\n",
    "                print('error in OLS')\n",
    "                print('coef_id',coef_id)\n",
    "                print('formula OLS',type(formula),formula)\n",
    "                #return pd.concat(final_results)\n",
    "                continue\n",
    "            ols.raise_on_perfect_prediction = False #preventing the perfect separation error\n",
    "            results = ols.fit(disp=False, maxiter=1000) #mehtod prevents singular matrix\n",
    "#            return results\n",
    "            results = results_fit_to_df(results)\n",
    "            results['standard_fitting'] = True\n",
    "\n",
    "            #If nan means no convergence bc singular matrix\n",
    "            #adding regularization\n",
    "            if 'nan' == pd.DataFrame(results)['z'].astype(str).iloc[2].strip():\n",
    "                try:\n",
    "                    results = ols.fit_regularized(method='l1', disp=False, maxiter=1000, alpha=0.3) #mehtod prevents singular matrix\n",
    "                    results = results_fit_to_df(results)\n",
    "                    results['standard_fitting'] = False        \n",
    "                except:\n",
    "                    #crashed the regularized\n",
    "                    counts +=1\n",
    "                    continue\n",
    "\n",
    "\n",
    "            results['snps'] = v\n",
    "            results['order'] = len(sp)\n",
    "            final_results.append(results)\n",
    "\n",
    "    final_results = pd.concat(final_results)\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ce16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo(string):\n",
    "    \n",
    "    string = ''.join([ x if ord(x)<90 else str(ord(x)-97) for x in string ])\n",
    "    string = string[:6]+'.'+string[6:].replace('HUMAN', '_HUMAN') #not sure these 6\n",
    "    return string\n",
    "    \n",
    "#undo('PacfbbCRYABHUMAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b0b7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 out of 374 drugs in split nr 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 3 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:1749: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/statsmodels/regression/linear_model.py:1749: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 6 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=18\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n",
      "/opt/homebrew/anaconda3/envs/vs05/lib/python3.8/site-packages/scipy/stats/stats.py:1603: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=8\n",
      "  warnings.warn(\"kurtosistest only valid for n>=20 ... continuing \"\n"
     ]
    }
   ],
   "source": [
    "#TODO: By compound\n",
    "\n",
    "#Looping over all drugs\n",
    "# drug_id\n",
    "# Other options:\n",
    "# - drug_name\n",
    "# - CHEMBL = Chemical compound ID\n",
    "#for compound_name, group in x.merge(y, left_on='Cell_Line', right_on='cell_line_name').groupby('drug_id'): # may require too much memory\n",
    "\n",
    "#Making a temp file to run all R stuff\n",
    "#DIR\n",
    "os.system(\"mkdir -p \"+dir_trees_tmp+f\"{split_nr}\")\n",
    "\n",
    "\n",
    "column_to_group = 'drug_id'\n",
    "drugs_list = y[column_to_group].sort_values().unique()\n",
    "drugs_list = [drugs_list[i] for i in range(len(drugs_list)) if i%n_splits==split_nr-1]\n",
    "i = -1\n",
    "all_drug_results = []\n",
    "for elm in drugs_list:\n",
    "    i+=1\n",
    "    \n",
    "    if i%10==0 or i<10: print(i,'out of',len(drugs_list), 'drugs in split nr', split_nr)\n",
    "\n",
    "    xy = x.merge(y[y[column_to_group]==elm], left_on='Cell_Line', right_on='cell_line_name')\n",
    "    #Enhancement: Remove peptides that are all zero \n",
    "    \n",
    "    # saving csv for R df\n",
    "    # file name is generic but we could personalize it\n",
    "    #DIR\n",
    "    xy.drop(columns=['Cell_Line', 'cell_line_name','drug_id']).fillna(0).rename(columns={'ln_IC50':'label'}).to_csv(dir_trees_tmp+f\"{split_nr}/data.csv\", index=False)\n",
    "\n",
    "    #Run the R script to generate the outputs\n",
    "    os.system(f\"{path_to_R} {path_to_ranger_script}  -w {working_dir} -c {split_nr} -n {n_trees} -t {mtry} -s {min_node} -d {max_depth}\")\n",
    "    \n",
    "    #load the R outputs (the trees, one file each), and convert it to VS look-alike and get interactions\n",
    "    interactions = {}\n",
    "    #DIR\n",
    "    trees = os.listdir(dir_trees_tmp+f\"{split_nr}\")\n",
    "    #files = [x for x in files if 'tree' in x]\n",
    "    for tree in trees:\n",
    "        if 'tree' not in tree: continue #if it is not a tree file ignore\n",
    "        #DIR\n",
    "        tree_json = from_table_to_json(pd.read_csv(os.path.join(dir_trees_tmp+str(split_nr),tree)))        \n",
    "        get_interactions(tree_json,[],interactions) #the interactions are found in \"interactions\"\n",
    "        \n",
    "    # Creating a df out of the interactions\n",
    "    df = pd.DataFrame({'variants':interactions.keys(),'repetitions':interactions.values()})\n",
    "    df['order'] = df.variants.apply(lambda x: x.count('+')+1)\n",
    "    \n",
    "    \n",
    "    tested_interactions = test_interactions_high(df, xy) #here you define which order of interactions you want to compute\n",
    "    tested_interactions['drug'] = elm\n",
    "    all_drug_results.append(tested_interactions)\n",
    "    break\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "final_results = pd.concat(all_drug_results)\n",
    "#DIR\n",
    "final_results.to_csv(working_dir+f\"final_results{split_nr}.tsv\", index=False, sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b111aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split  1 out of  2  is DONE\n"
     ]
    }
   ],
   "source": [
    "print('Split ',split_nr, 'out of ',n_splits,' is DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9fd0d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fr = [x for x in os.listdir(working_dir) if 'final_results' in x and 'final_results_all.tsv' not in x]\n",
    "if len(fr) == n_splits:\n",
    "    df = pd.concat([pd.read_csv(os.path.join(working_dir,final_result)) for final_result in fr])\n",
    "    #df = df[df.coef_id.apply(lambda x: x.count(':'))==df.snps.apply(lambda x: x.count('+'))] #this keeps only the interactions\n",
    "    df.to_csv(working_dir+\"final_results_all.tsv\", index=False, sep='\\t')\n",
    "\n",
    "    print('All jobs finished successfully!\\n final_results_all.tsv has all the aggregated output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
